Day 1:
1. Set up the github repository
    a) new environemt
        - "conda create -p venv python==3.9 -y"
        - do "y" so it loads when you open it and you don't need to continue to grant access
        - "conda activate /Users/hilpresent/Desktop/Portfolio/project_ml-project/venv" to activate virtual environment
    b) clone repository and sync with github
        - git init
        - git add README.md
            - I created an actual file in VS Code first and then executed the "git add README.md" command
        - git commit -m "first commit"
        - git branch -M main
        - git remote add origin https://github.com/hilpresent/ml-project.git
            - adding this makes sure local device is in sync with github repo
            - "git remote -v" will confirm that this project folder has been properly synced with your github repo
        - git push -u origin main
            - "git config --global user.email" to set email address if this is your first repo
            - can change email by typing "git config --global user.email <new_email_address>"
        - create .gitignore on github site
            - select "add a new file" and name it ".gitignore"
            - choose python as template
            - commit change
            - git pull on your local directory to get the .gitignore file
    c) setup.py and requirements.txt
        - create new file called setup.py
            - responsible in making machine learning application as a package
            - allows anybody to install this application as a package and use it themselves
        - create new file called requirements.txt
            - will hold information on all of the packages and libraries we will be using throughout this project
        - see files to see what needs to be added
        - had to run "pip install --force-reinstall -r requirements.txt" in order to install in my venv instead of my local comp
        - once things are running properly, "pip freeze > requirements.txt" to save current version of packages and libraries
2. src folder and build the package
    a) create "src" folder and add "__init__.py" file to tell setup.py where to read requirements from
    b) run "pip install -r requirements.txt" to install libraries necessary for this project
            - indicates to run setup.py to build the project   
            - generates "ml_project.egg-info" folder with information on the project
            - top_level.txt -> finds src folder because that contains "__init__.py" file
            - requires.txt -> list of all requirements sans "-e ."
3. don't forget to "git add ." and "git commit" and "git push -u origin main"!
    a) note: the "-u" in "git push -u origin main" sets the upstream branch 
        - means git remembers the branch you are tracking
        - don't need to keep indicating remote and branch each time
        - streamlines for future use (can just say "git push" and "git pull")

Day 2:
1. project structure
    a) will be doing manually to ensure I understand everything
    b) this can also be done manually  
        - can create and code something like "template.py" and automatically the entire folder structure will be created

    - create "components" folder in "src" folder
    - add "__init__.py" file to allow components to be created as a package
        - allows to be exported (imported to some other file location)
    - will contain all of the modules we will be using 
        - ex: data_ingestion -> reading the data from some database
    - add "data_ingestion.py" file to components
    - add "data_transformation.py" file to components
    - add "model_trainer.py" file to components
        - will including pushing model pkl file to the cloud

    - create "pipeline" folder in "src" folder
    - create "train_pipeline.py" file in pipeline
        - will have all code for the training pipeline
        - will try to trigger/call all of the training components/modules
    - create "predict_pipeline.py" in pipeline 
        - for the new data
        - will call all of the prediction modules that we code
    - add __init__.py so that everything here can be imported!!

    c) create logger.py file for logging
    d) create exception.py for exception handling
    e) create utils.py  
        - for any functionalities that we're writing in a common way
        - ex: reading a dataset from a database, create mongodb client in utils.py
        - ex: saving a model into the cloud, write the code in utils.py

2. exception handling
    - going to write own custom exception
    - see exception.py for the code and notes on what each line is doing

3. logging
    - created a function to log any errors that occur
    - see logging.py for the code and notes on what each line is doing


Day 3:
1. project problem structure 
    - what kind of dataset should we start with?
        - very important question to start with this question
    - project basis: student performance indicators
    - project may sound simple, but it's all encompassing
        - features of all types (categorical, numerical, NaN, etc.)
        - want to see how we can perform feature engineering to begin with
    - want to take the jupyter notebook code we'd normally use and convert it to modular code
2. EDA
    - create EDA.ipynb notebook
    - when starting to write code, will need to connect to a Python kernel and install the kernel for this project's venv in that notebook
    - run "python -m ipykernel install --user --name=ml-project-venv --display-name "Python (ml-project-venv)"" in order to properly set up venv kernel
3. model training
    - go through model training notebook similar to the EDA notebook above
    - once done, we will get starting on moving all of it over to our src code to actually implement the code!

Day 4: Data Ingestion Implementation
1. Convert to CICD (continuous integration/continuous deploymnet) pipeline
    - we want to take our work from our notebooks and convert it to actual code that can be running continuously whenever we want
    - read the data from some data source (from Big Data team at company or cloud team, etc.)
    - create configuration classes such as DataIngestionConfig
        - point of condig classes is to centralize and manage configurations settings related to a specific component of the data processing pipeline
        - easier to modify and reuse as needed
    - add a @dataclass decorator
        - `@dataclass` decorator in Python auto generates special methods for the class (such as `__init__()`)
        - when you use `@dataclass` it creates an __init__ method for you, so you don't need to manually write it out and use self to assign parameters to instance vars
        - makes code a lot more concise
